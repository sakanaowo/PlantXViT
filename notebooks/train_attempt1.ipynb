{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ah4BBtrlLcFG",
        "outputId": "5dcd6ab2-9401-4316-ddc5-c980999a0025"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'PlantXViT'...\n",
            "remote: Enumerating objects: 50280, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
            "remote: Total 50280 (delta 0), reused 5 (delta 0), pack-reused 50271 (from 1)\u001b[K\n",
            "Receiving objects: 100% (50280/50280), 1.66 GiB | 61.52 MiB/s, done.\n",
            "Resolving deltas: 100% (30385/30385), done.\n",
            "Updating files: 100% (50019/50019), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/sakanaowo/PlantXViT"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "preprocess apple here\n"
      ],
      "metadata": {
        "id": "zMd5inpErA-r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%cd PlantXViT"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vDUFG53wrADo",
        "outputId": "3e7d80ee-6f92-41ed-b4d8-cf4ab8564106"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/PlantXViT\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from utils.config_loader import load_config\n",
        "\n",
        "config = load_config('configs/config.yaml')"
      ],
      "metadata": {
        "id": "8F581vbvrK5-"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pandas"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "noK0Wg5ErxvO",
        "outputId": "2c3d93eb-a66c-4042-d3a9-fd21f43ae31b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#import lib\n",
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "from torchvision import transforms\n",
        "import pickle"
      ],
      "metadata": {
        "id": "-HuoRIBnrlr2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#load config\n",
        "apple_config = config['dataset']['apple']\n",
        "img_dir = apple_config['data_dir']\n",
        "csv_path = apple_config['csv_path']\n",
        "label_encoder_path = apple_config['label_encoder']\n",
        "image_size = tuple(apple_config['image_size'])"
      ],
      "metadata": {
        "id": "F6HSrtGmstbP"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# read csv, preprocess label\n",
        "df = pd.read_csv(csv_path)\n",
        "df['label'] = df[['healthy', 'multiple_diseases', 'rust', 'scab']].idxmax(axis=1)"
      ],
      "metadata": {
        "id": "1NxcIg8SsvwF"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# encode label\n",
        "label_encoder = LabelEncoder()\n",
        "df['label'] = label_encoder.fit_transform(df['label'])\n"
      ],
      "metadata": {
        "id": "yS6BNtT1tBGD"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# save label encoded\n",
        "os.makedirs(os.path.dirname(label_encoder_path), exist_ok=True)\n",
        "with open(label_encoder_path, 'wb') as f:\n",
        "    pickle.dump(label_encoder, f)"
      ],
      "metadata": {
        "id": "7ReKBJfTtHGl"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(label_encoder_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "huq0sZa5tJjl",
        "outputId": "38e14b60-b15d-4d8d-9afe-a6f61638eadd"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./data/processed/apple_label_encoder.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split train/val\n",
        "train_df, val_df = train_test_split(df, test_size=0.2,\n",
        "                                    stratify=df['label'],\n",
        "                                    random_state=42)\n",
        "\n",
        "# Encode labels\n",
        "label_encoder = LabelEncoder()\n",
        "train_df['label_idx'] = label_encoder.fit_transform(train_df['label'])\n",
        "val_df['label_idx'] = label_encoder.transform(val_df['label'])"
      ],
      "metadata": {
        "id": "yhYjqQxVtWli"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "transform from here\n"
      ],
      "metadata": {
        "id": "TrAViEZztxY3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image_dir = \"./data/raw/plant-pathology-2020-fgvc7/images\"\n",
        "train_df = pd.read_csv(\"./data/processed/apple_train.csv\")\n",
        "val_df = pd.read_csv(\"./data/processed/apple_val.csv\")"
      ],
      "metadata": {
        "id": "KS7zzxEU7Cvm"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize(image_size),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "class AppleDataset(Dataset):\n",
        "    def __init__(self, dataframe, image_dir, transform):\n",
        "        self.df = dataframe\n",
        "        self.image_dir = image_dir\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "        img_path = os.path.join(self.image_dir, row['image_id'] + \".jpg\")\n",
        "        image = Image.open(img_path).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "        label = torch.tensor(row['label_idx'])\n",
        "        return image, label\n"
      ],
      "metadata": {
        "id": "y_N1XViJt0dp"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Data loader\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "batch_size = config[\"training\"][\"batch_size\"]\n",
        "\n",
        "train_dataset = AppleDataset(train_df, img_dir, transform)\n",
        "val_dataset = AppleDataset(val_df, img_dir, transform)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size)\n"
      ],
      "metadata": {
        "id": "giA0g6aAt4It"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1779c89"
      },
      "source": [
        "# save preprocessed train and val csv\n",
        "os.makedirs(\"./data/processed\", exist_ok=True)\n",
        "\n",
        "train_df[['image_id', 'label', 'label_idx']].to_csv(\"./data/processed/apple_train.csv\", index=False)\n",
        "val_df[['image_id', 'label', 'label_idx']].to_csv(\"./data/processed/apple_val.csv\", index=False)"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Building model here\n"
      ],
      "metadata": {
        "id": "Eiu_Y-aLy9Rc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "from torchvision.models import VGG16_Weights"
      ],
      "metadata": {
        "id": "3Y4Lw7kJy862"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# inception block\n",
        "class InceptionBlock(nn.Module):\n",
        "    def __init__(self, in_channels):\n",
        "        super().__init__()\n",
        "        self.branch1x1 = nn.Conv2d(in_channels, 128, kernel_size=1)\n",
        "\n",
        "        self.branch3x3 = nn.Sequential(\n",
        "            nn.Conv2d(in_channels, 128, kernel_size=(1, 3), padding=(0, 1)),\n",
        "            nn.Conv2d(128, 128, kernel_size=(3, 1), padding=(1, 0)),\n",
        "        )\n",
        "\n",
        "        self.branch_pool = nn.Sequential(\n",
        "            nn.MaxPool2d(kernel_size=3, stride=1, padding=1),\n",
        "            nn.Conv2d(in_channels, 128, kernel_size=1),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b1 = self.branch1x1(x)\n",
        "        b2 = self.branch3x3(x)\n",
        "        b3 = self.branch_pool(x)\n",
        "        return torch.cat([b1, b2, b3], dim=1)\n",
        "\n"
      ],
      "metadata": {
        "id": "4kqXpXDxxdp-"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# patch embedding: split patch -> Linear\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, in_channels, patch_size=5, emb_size=16):\n",
        "        super().__init__()\n",
        "        self.patch_size = patch_size\n",
        "        self.emb_size = emb_size\n",
        "        self.proj = nn.Linear(in_channels * patch_size * patch_size, emb_size)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, C, H, W = x.shape\n",
        "        x = x.unfold(2, self.patch_size, self.patch_size).unfold(3, self.patch_size, self.patch_size)\n",
        "        x = x.permute(0, 2, 3, 1, 4, 5).contiguous()\n",
        "        x = x.view(B, -1, C * self.patch_size * self.patch_size)\n",
        "        return self.proj(x)  # shape: (b,num patches,emb size)\n"
      ],
      "metadata": {
        "id": "xWs_8ua_zDKp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -------- Transformer Encoder Block (ViT block) --------\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, emb_size=16, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(emb_size)\n",
        "        self.attn = nn.MultiheadAttention(emb_size, num_heads=2, batch_first=True)\n",
        "        self.norm2 = nn.LayerNorm(emb_size)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(emb_size, emb_size * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Linear(emb_size * 2, emb_size),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_attn, _ = self.attn(self.norm1(x), self.norm1(x), self.norm1(x))\n",
        "        x = x + x_attn\n",
        "        x = x + self.mlp(self.norm2(x))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "LRFT1JQmzFKL"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -------- PlantXViT Model --------\n",
        "class PlantXViT(nn.Module):\n",
        "    def __init__(self, num_classes=4, patch_size=5, emb_size=16, num_blocks=4, dropout=0.1):\n",
        "        super().__init__()\n",
        "\n",
        "        # VGG16 (2 blocks)\n",
        "        vgg = models.vgg16(weights=VGG16_Weights.DEFAULT)\n",
        "        self.vgg_block = nn.Sequential(*vgg[:10])  # output: (B, 128, 56, 56)\n",
        "\n",
        "        # Inception-like block → (B, 384, 56, 56)\n",
        "        self.inception = InceptionBlock(in_channels=128)\n",
        "\n",
        "        # Patch Embedding → (B, 121, 16)\n",
        "        self.patch_embed = PatchEmbedding(in_channels=384, patch_size=patch_size, emb_size=emb_size)\n",
        "\n",
        "        # Transformer blocks\n",
        "        self.transformer = nn.Sequential(*[TransformerBlock(emb_size, dropout) for _ in range(num_blocks)])\n",
        "\n",
        "        # Classification head\n",
        "        self.norm = nn.LayerNorm(emb_size)\n",
        "        self.global_pool = nn.AdaptiveAvgPool1d(1)  # (B, emb_size, 1)\n",
        "        self.classifier = nn.Linear(emb_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.vgg_block(x)  # (B, 128, 56, 56)\n",
        "        x = self.inception(x)  # (B, 384, 56, 56)\n",
        "        x = self.patch_embed(x)  # (B, 121, 16)\n",
        "        x = self.transformer(x)  # (B, 121, 16)\n",
        "        x = self.norm(x)  # (B, 121, 16)\n",
        "        x = x.permute(0, 2, 1)  # (B, 16, 121)\n",
        "        x = self.global_pool(x).squeeze(-1)  # (B, 16)\n",
        "        return self.classifier(x)  # (B, num_classes)\n"
      ],
      "metadata": {
        "id": "XCkbf_JSzHJM"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# test model here\n",
        "model = PlantXViT(\n",
        "    num_classes=4,\n",
        "    patch_size=5,\n",
        "    emb_size=16,\n",
        "    num_blocks=4,\n",
        "    dropout=0.1\n",
        ")\n",
        "\n",
        "dummy_input = torch.randn(1, 3, 224, 224)\n",
        "output = model(dummy_input)\n",
        "print(\"Output shape:\", output.shape)  # 👉 torch.Size([1, 4])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K_D1cbQUzJhl",
        "outputId": "087e3527-e8a7-4af7-9b4f-462a4ad8ce3a"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/vgg16-397923af.pth\" to /root/.cache/torch/hub/checkpoints/vgg16-397923af.pth\n",
            "100%|██████████| 528M/528M [00:02<00:00, 209MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output shape: torch.Size([1, 4])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "trying to train here"
      ],
      "metadata": {
        "id": "MiOTu-Rr10Bx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_one_epoch(model, dataloader, criterion, optimizer, device):\n",
        "    model.train()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    for images, labels in dataloader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * images.size(0)\n",
        "        _, preds = torch.max(outputs, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "    return epoch_loss, accuracy\n"
      ],
      "metadata": {
        "id": "__Vdh1Jr11_o"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate(model, dataloader, criterion, device):\n",
        "    model.eval()\n",
        "    running_loss, correct, total = 0.0, 0, 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for images, labels in dataloader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            running_loss += loss.item() * images.size(0)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "    epoch_loss = running_loss / total\n",
        "    accuracy = correct / total\n",
        "    return epoch_loss, accuracy\n"
      ],
      "metadata": {
        "id": "OVPpjLsj16rM"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(config['output']['model_path'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BeXU-b_B2ALk",
        "outputId": "09890ac7-c5e8-49b5-eed1-2e1cb7f8bb8a"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "./outputs/models/plantxvit_best.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "\n",
        "def train_model(\n",
        "    model, train_loader, val_loader,\n",
        "    criterion, optimizer,\n",
        "    num_epochs, device,\n",
        "    save_path=\"./outputs/models/plantxvit_best.pth\"\n",
        "):\n",
        "    best_val_loss = float('inf')\n",
        "    history = {\"train_loss\": [], \"val_loss\": [], \"train_acc\": [], \"val_acc\": []}\n",
        "\n",
        "    for epoch in range(1, num_epochs + 1):\n",
        "        train_loss, train_acc = train_one_epoch(model, train_loader, criterion, optimizer, device)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n",
        "\n",
        "        print(f\"Epoch [{epoch}/{num_epochs}] \"\n",
        "              f\"Train Loss: {train_loss:.4f}, Acc: {train_acc:.4f} | \"\n",
        "              f\"Val Loss: {val_loss:.4f}, Acc: {val_acc:.4f}\")\n",
        "\n",
        "        history[\"train_loss\"].append(train_loss)\n",
        "        history[\"val_loss\"].append(val_loss)\n",
        "        history[\"train_acc\"].append(train_acc)\n",
        "        history[\"val_acc\"].append(val_acc)\n",
        "\n",
        "        # Save best model\n",
        "        if val_loss < best_val_loss:\n",
        "            best_val_loss = val_loss\n",
        "            os.makedirs(os.path.dirname(save_path), exist_ok=True)\n",
        "            torch.save(model.state_dict(), save_path)\n",
        "            print(\"✅ Saved best model.\")\n",
        "\n",
        "    return history\n"
      ],
      "metadata": {
        "id": "pSOwGls318ZX"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device.type)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4Zp_OeGX2rTa",
        "outputId": "3e6fbfcb-9fff-4298-9246-5865b5a59bc7"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# from model import PlantXViT  # hoặc copy trực tiếp class từ các cell trên\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "device = torch.device(\"cuda\")\n",
        "\n",
        "model = PlantXViT(num_classes=4).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gy4GxT0e2fPP",
        "outputId": "b519a75a-baff-4493-c5b3-be9bb243a9f7"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "history = train_model(\n",
        "    model=model,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=50,\n",
        "    device=device,\n",
        "    save_path=\"./outputs/models/plantxvit_best.pth\"\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7uQ77BMT7YJh",
        "outputId": "4dee9999-0d1c-437c-b99c-77d155180e1f"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/50] Train Loss: 1.2122, Acc: 0.4093 | Val Loss: 1.1135, Acc: 0.5260\n",
            "✅ Saved best model.\n",
            "Epoch [2/50] Train Loss: 1.0404, Acc: 0.5714 | Val Loss: 1.0010, Acc: 0.6219\n",
            "✅ Saved best model.\n",
            "Epoch [3/50] Train Loss: 0.9143, Acc: 0.6641 | Val Loss: 0.9283, Acc: 0.6466\n",
            "✅ Saved best model.\n",
            "Epoch [4/50] Train Loss: 0.8451, Acc: 0.6923 | Val Loss: 0.8129, Acc: 0.7315\n",
            "✅ Saved best model.\n",
            "Epoch [5/50] Train Loss: 0.7735, Acc: 0.7424 | Val Loss: 0.7609, Acc: 0.7096\n",
            "✅ Saved best model.\n",
            "Epoch [6/50] Train Loss: 0.7011, Acc: 0.7589 | Val Loss: 0.6491, Acc: 0.8082\n",
            "✅ Saved best model.\n",
            "Epoch [7/50] Train Loss: 0.5787, Acc: 0.8187 | Val Loss: 0.5140, Acc: 0.8630\n",
            "✅ Saved best model.\n",
            "Epoch [8/50] Train Loss: 0.4586, Acc: 0.8791 | Val Loss: 0.4547, Acc: 0.8822\n",
            "✅ Saved best model.\n",
            "Epoch [9/50] Train Loss: 0.3316, Acc: 0.9210 | Val Loss: 0.3782, Acc: 0.9096\n",
            "✅ Saved best model.\n",
            "Epoch [10/50] Train Loss: 0.2677, Acc: 0.9354 | Val Loss: 0.3527, Acc: 0.9068\n",
            "✅ Saved best model.\n",
            "Epoch [11/50] Train Loss: 0.2101, Acc: 0.9547 | Val Loss: 0.3507, Acc: 0.8904\n",
            "✅ Saved best model.\n",
            "Epoch [12/50] Train Loss: 0.1949, Acc: 0.9567 | Val Loss: 0.3370, Acc: 0.9014\n",
            "✅ Saved best model.\n",
            "Epoch [13/50] Train Loss: 0.1616, Acc: 0.9670 | Val Loss: 0.3539, Acc: 0.8932\n",
            "Epoch [14/50] Train Loss: 0.1378, Acc: 0.9766 | Val Loss: 0.3369, Acc: 0.9014\n",
            "✅ Saved best model.\n",
            "Epoch [15/50] Train Loss: 0.1461, Acc: 0.9684 | Val Loss: 0.3305, Acc: 0.9041\n",
            "✅ Saved best model.\n",
            "Epoch [16/50] Train Loss: 0.1293, Acc: 0.9760 | Val Loss: 0.3125, Acc: 0.9178\n",
            "✅ Saved best model.\n",
            "Epoch [17/50] Train Loss: 0.1000, Acc: 0.9849 | Val Loss: 0.3839, Acc: 0.8904\n",
            "Epoch [18/50] Train Loss: 0.0828, Acc: 0.9876 | Val Loss: 0.2974, Acc: 0.9205\n",
            "✅ Saved best model.\n",
            "Epoch [19/50] Train Loss: 0.0604, Acc: 0.9966 | Val Loss: 0.3136, Acc: 0.9178\n",
            "Epoch [20/50] Train Loss: 0.0485, Acc: 0.9993 | Val Loss: 0.3076, Acc: 0.9178\n",
            "Epoch [21/50] Train Loss: 0.0429, Acc: 0.9993 | Val Loss: 0.3092, Acc: 0.9123\n",
            "Epoch [22/50] Train Loss: 0.0377, Acc: 0.9993 | Val Loss: 0.3198, Acc: 0.9151\n",
            "Epoch [23/50] Train Loss: 0.0337, Acc: 1.0000 | Val Loss: 0.3077, Acc: 0.9205\n",
            "Epoch [24/50] Train Loss: 0.0307, Acc: 1.0000 | Val Loss: 0.3134, Acc: 0.9233\n",
            "Epoch [25/50] Train Loss: 0.0283, Acc: 1.0000 | Val Loss: 0.3224, Acc: 0.9178\n",
            "Epoch [26/50] Train Loss: 0.0262, Acc: 1.0000 | Val Loss: 0.3245, Acc: 0.9178\n",
            "Epoch [27/50] Train Loss: 0.0243, Acc: 1.0000 | Val Loss: 0.3308, Acc: 0.9178\n",
            "Epoch [28/50] Train Loss: 0.0226, Acc: 1.0000 | Val Loss: 0.3385, Acc: 0.9178\n",
            "Epoch [29/50] Train Loss: 0.0210, Acc: 1.0000 | Val Loss: 0.3415, Acc: 0.9151\n",
            "Epoch [30/50] Train Loss: 0.0196, Acc: 1.0000 | Val Loss: 0.3483, Acc: 0.9151\n",
            "Epoch [31/50] Train Loss: 0.0183, Acc: 1.0000 | Val Loss: 0.3532, Acc: 0.9151\n",
            "Epoch [32/50] Train Loss: 0.0171, Acc: 1.0000 | Val Loss: 0.3605, Acc: 0.9151\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-43-c6056b051382>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m history = train_model(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mval_loader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mcriterion\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-de73bca1744e>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_loader, val_loader, criterion, optimizer, num_epochs, device, save_path)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mtrain_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_one_epoch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mval_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_acc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-26-d5f6ef7f9fe0>\u001b[0m in \u001b[0;36mtrain_one_epoch\u001b[0;34m(model, dataloader, criterion, optimizer, device)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mcorrect\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}